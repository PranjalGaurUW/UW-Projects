---
title: "STAT-847-Final-Project"
author: "Pranjal Gaur"
date: "20/04/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(ggplot2)
library(tidyverse)
library(reshape)
library(prob)
library(syuzhet)
library(plyr)
```

## Reading Data

Reading the gamelog data and checking dimensions

```{r message=FALSE, warning=FALSE}
match_data= read.csv("Gamelog T20I Stat 847.csv")
head(match_data)
dim(match_data)

data1=match_data
```

## Data Cleaning

Analyzing the data and filtering out match_ids with more than 1 match data, missing leagues, abnormal wicket count(like 24), more than two teams in a game, etc.

```{r message=FALSE, warning=FALSE}
sort(unique(match_data$MatchNo))
summary(match_data$NumOutcome)
```

As we see there exists a match with 2015 runs which is an anomaly, so we filter it out.

```{r message=FALSE, warning=FALSE}
rowsToExclude=which(data1$NumOutcome==2015)
data1=match_data[-rowsToExclude,]
```

Similarly, we exclude rows containing NA, considering these to be case of MCAR. Also, we remove rows containing empty fields in Format.

```{r message=FALSE, warning=FALSE}
data1=data1[!(is.na(data1$MatchNo)),]
data1=data1[!(is.na(data1$NumOutcome)),]
data1=data1[-which(data1$Format ==" "),]
```

Finally, we proceed to find and remove data for matches with abnormal runs or single match id containing duplicate or multiple matches data.

```{r message=FALSE, warning=FALSE}
match_ids_faulty= c()
match_ids_faulty = c(match_ids_faulty, unique(data1[which(data1$Wickets>9), 2]))

faulty_matches = c(191, 440, 200903, 200906, 200933,200938,200947,200948,201103,
                   201109, 201112,201128,201131,201211,201228,201258
                   ,201273,201275,201536,201554)
match_ids_faulty = c(match_ids_faulty, faulty_matches)
data2=data1[-which(data1$MatchNo %in% match_ids_faulty),]

total_runs_per_match_clean_data = ddply(data2, 
                                        .(MatchNo),
                                        summarize,
                                        total_runs = sum(NumOutcome[which(
                                          NumOutcome >=0 & NumOutcome<=7)])
)
summary(total_runs_per_match_clean_data)
matches_with_redundant_records = total_runs_per_match_clean_data[which(
  total_runs_per_match_clean_data$total_runs > 465),1]
data2=data2[-which(data2$MatchNo %in% matches_with_redundant_records),]

head(data2)
```



## Part 1. Answer

For first ggplot, we slice the data to get insights about how teams approach six-hitting through the course of an inning in IPL, we plot the heat map showing percent of total sixes hit in each over by IPL teams.

```{r message=FALSE, warning=FALSE}
# Analysis of percentage of total sixes hit in each over by an IPL team

 #only including IPL teams
df_split_1=ddply(subset(data2, Format == "IPL" & !(TeamBowling %in% c("IND"))),
                 .(TeamBowling,Over),
                 summarize,
                 total_wickets = mean(NumOutcome == 6)*100
)

ggplot(df_split_1,aes(y=TeamBowling,x=Over, fill=total_wickets)
       ,xlab="Over"
       ,ylab="Team") + geom_tile() +scale_fill_gradient(low = "green",
                                                        high = "dark green")
```

As we see above, maximum of sixes are preferred to hit in final 3 overs, which makes sense as teams are not afraid to take more risk towards final 3 overs of the innings.

For second ggplot, we slice the data to find total wickets taken in each over across two formats - namely, IPL and T20 Internationals.
This analysis again provides insights into regarding how bowler-friendly are the pitches in IPL vs T20I based on how many wickets fall in each over across two leagues.

```{r message=FALSE, warning=FALSE}
#total wickets taken in each over across format
df_split_2=ddply(data2, 
                 .(Over,Format),
                 summarize,
                 total_wickets = length(which(NumOutcome == -1))
)

ggplot(df_split_2,aes(x=Over,y=Format, fill=total_wickets)
       ,xlab="Over"
       ,ylab="Inning") +  geom_tile() + scale_fill_gradient(low = "orange",
                                                            high = "maroon")

```

Further, analyzing the data to find total sixes hit in each over across two innings. This analysis can help to understand the difference in risk approach of batsmen across innings and different overs.

```{r message=FALSE, warning=FALSE}
#total sixes hit in each over in an inning
df_split_3=ddply(data2, 
               .(Over,Inning),
               summarize,
               total_six_hit = length(which(NumOutcome == 6))
               )

ggplot(df_split_3,aes(x=Over,y=as.factor(Inning), fill=total_six_hit)
       ,xlab="Over"
       ,ylab="Inning") +  geom_tile()

```


## Part 2. Answer

Generating highlights of second inning data based on sum of emotional valence and resource usage difference between subsequent balls calculated for each ball to assign a score.

(Please follow comments for code explanation)

```{r message=FALSE, warning=FALSE}
DLS = read.csv("DLS_T20.csv")[,-1]
result_highlights = data.frame()
match_ids = unique(data2$MatchNo)

for(match in match_ids) #repeat for each match
{
  resource_used_vector = c() #vector that contains resource usage difference for each ball
  resource_left_vector = c() #vector that contains resource left after each ball
  runs_needed_to_keep_up=c() #vector that contains runs needed after each ball
  
  data_first_inning=subset(data2,MatchNo ==match & Inning==1) #first inning data
  target = by(data_first_inning$NumOutcome>=0 & data_first_inning$NumOutcome<=7,
              data_first_inning$Inning, sum) #target 
  data_second_inning=subset(data2,MatchNo == match & Inning==2) #second inning data
  
  sample_text <- data_second_inning$FullNotes #capturing commentary/FullNotes data
  
  #performing sentiment analysis by using afinn method
  afinn_vector=get_sentiment(sample_text, method="afinn", lang="english") 
  
  #for each ball bowled in second inning, calculate resource usage and resource left
  for(i in 1:nrow(data_second_inning)) 
  {
    resource_left = (1-(data_second_inning[i,7]/6))*DLS[data_second_inning[i,6]+1,
                                                    data_second_inning[i,21]+1]
    +data_second_inning[i,7]/6*DLS[data_second_inning[i,6]+2,
                                   data_second_inning[i,21]+1]                                                                     
    resource_used = DLS[data_second_inning[i,6]+1,data_second_inning[i,21]+1] -
      resource_left
    runs_needed_to_keep_up = c(runs_needed_to_keep_up,
                               round(resource_used * target, 3))
     # round to 4 digits
    resource_used_vector = c(resource_used_vector, round(resource_used, 4))
    resource_left_vector = c(resource_left_vector, round(resource_left, 4)) 
  }
  #initializing vector to contain score for each ball
  ball_score=c(as.numeric(afinn_vector[1])) 
  highlight_balls = data.frame(cbind(data_second_inning,ball_score[1]))
   #renaming column to -> Score
  colnames(highlight_balls)[dim(highlight_balls)[2]] = "Score"
  
  for(i in 2:length(afinn_vector)) #for each ball
  {
    #calculating emotional valence through sentiment analysis
    emotional_valence = abs(afinn_vector[i]) 
    #for each ball bowled in second inning, calc difference in resource usage and resource left
    resource_usage_difference = abs(resource_used_vector[i-1] - resource_used_vector[i])
    resource_left_difference = abs(resource_left_vector[i] - resource_left_vector[i-1])
    
    if(data_second_inning[i,15] == -1 ) #for wicket, assign higher score
      ball_score = c(ball_score, 2.5*(emotional_valence) + 100*resource_usage_difference)
    
    #for fours (num outcome = 5 included for four on a no-ball or wide), assign higher score
    else if(data_second_inning[i,15] >= 4  & data_second_inning[i,15] < 6) 
      ball_score = c(ball_score, 1.5*(emotional_valence) + 100*resource_usage_difference)
    #for sixes (num outcome = 7 included for six on a no-ball or wide), assign higher score
    else if(data_second_inning[i,15] >= 6)
      ball_score = c(ball_score, 2.5*(emotional_valence) + 100*resource_usage_difference)
    else #assign lower score for other balls
      ball_score = c(ball_score, (emotional_valence + resource_usage_difference))
    highlight_balls[nrow(highlight_balls)+1,] = c(data_second_inning[i,], ball_score[i])
    
  }
  Final_highlight_balls = highlight_balls
  Final_highlight_balls[,22] = highlight_balls[,22]
  
  #sorting in decreasing order of score assigned to each ball
  Final_highlight_balls = Final_highlight_balls[order(-Final_highlight_balls$Score),]
  #taking top 20 balls with highest score in our highlights
  Final_highlight_balls = Final_highlight_balls[1:20,]
  #arranging rows in increasing order based on overs and balls
  Final_highlight_balls = Final_highlight_balls[order(Final_highlight_balls$Over,
                                                      Final_highlight_balls$Ball),]
  
  result_highlights = rbind(result_highlights,Final_highlight_balls)
  
}
```

Here, result_highlights is the dataframe containing 20 ball highlights/turning-points of each match provided in the dataset.

As a sample, we see below the 20-ball highlights for the match between Aus and BD.

```{r message=FALSE, warning=FALSE}
result_highlights[which(result_highlights$MatchNo==161),]
```

## Part 3. Answer

Here, we are trying to analyze cluster of matches based on total wickets taken and total sixes hit during the match. We try to visualize the data using contour plot and scatter plot. We find that 10 sixes and around 12 wickets per match are the dominant figures in our data as seen in our plots below:

Scatter Plot

```{r message=FALSE, warning=FALSE}
cluster_df = data2
df_matches = ddply(cluster_df, "MatchNo", summarize,
                    total_wickets = length(which(NumOutcome == -1)),
                    total_six = length(which(NumOutcome == 6 | NumOutcome ==7)),
                    total_four = length(which(NumOutcome == 4 | NumOutcome ==5)),
                    total_runs = sum(pmax(NumOutcome, 0, na.rm=TRUE))
)

gr1 <- ggplot(df_matches, aes(x = total_six,  y = total_wickets)) +
  geom_point() +
  ylab("wickets taken during the match") +
  xlab("sixes hit during the match")
plot(gr1)
```

2D Contour Plot

```{r message=FALSE, warning=FALSE}
gr2 <- ggplot(df_matches, aes(x = total_six,  y = total_wickets)) +
  geom_density_2d_filled() +
  ylab("wickets taken during the match") +
  xlab("sixes hit during the match")
plot(gr2)
```

Now, we proceed with K-means clustering. We look at within sum of squares distance before deciding on the number of clusters.

```{r message=FALSE, warning=FALSE}
df_shot_risk = subset(df_matches, select = c(total_six, total_wickets))
wssd <- rep(NA,9)
for(k in 2:10) {
  shot_clust <- kmeans(df_shot_risk, centers = k)
  wssd[k-1] <- shot_clust$tot.withinss
}

centers <- 2:10
dat <- data.frame(centers, wssd)
gr3 <- ggplot(dat, aes(x=centers, y=wssd)) +
  geom_line() +
  geom_point() +
  xlab("number of clusters") +
  ylab("WSSD")
plot(gr3)
```

From the elbow plot, either 5 or 6 clusters provides an ideal balance between parsimony and goodness-of-fit. Weâ€™ll try 5 clusters for our initial analysis. This is because this is the point where diminishing returns are no longer worth the additional cost.

```{r message=FALSE, warning=FALSE}
# We can take 5 clusters in our case as can be seen from elbow chart

shot_clust_5 <- kmeans(df_shot_risk, centers = 5) 
shot_clust_5$centers
```

The relative amounts of diffusion and the sizes of each of these clusters is as below:

```{r message=FALSE, warning=FALSE}
msd <- sqrt(shot_clust_5$withinss / shot_clust_5$size)
msd

shot_clust_5$size
```

## Part 4. Answer

Using loss function, we come up with a new DLS table based on the dataset provided. Care has been taken to ensure it follows the following attributes: 
1. ranges 0 to 1
2. monotonic in wickets and in overs
3. has some non-linearity
4. only based on overs and wicket

```{r message=FALSE, warning=FALSE}
DLT = read.csv("DLS_T20.csv")[,-1]
dls_df = data.frame(subset(data2, Inning==1)) #First inning data filtered

dls_df$over2 = dls_df[,6] + dls_df[,7]/6 #reading overs column
dls_df$Nruns = pmax(dls_df[,15], 0) #reading runs column

dls_df_result = data.frame()

#function to calculate proportion of target achieved in each ball
func_calc_prop = function(match) 
{
  df_temp = data.frame()
  df_temp = data.frame(subset(dls_df, MatchNo==match))
  target = sum(df_temp$Nruns) #target
  
  #using cumulative sum to calc prop of target achieved after each ball
  for(i in 1:nrow(df_temp)) 
  {
    df_temp$cum = cumsum(df_temp$Nruns)
    df_temp$prop[i] = df_temp$cum[i]/target
  }
  return(rbind(dls_df_result,df_temp))
}

for(match in match_ids) #calling above function
  dls_df_result = func_calc_prop(match)

head(dls_df_result)

#preparing optimization 

over2 = dls_df_result$over2
wicket = dls_df_result$Wickets
prop = dls_df_result$prop

#loss function
loss_function = function(x, prop) {
  A = x[1]
  B = x[2]
  C = x[3]
  D = x[4]
  prop_smooth = A*over2^2 + B*(10-wicket)^2 + C*over2^3 + D*over2*(10-wicket)
  error = sum((prop - prop_smooth)^2)
  return(error)
}


best_params = optim(par=c(0,0,0,0), loss_function, prop=prop)$par
A = best_params[1]
B = best_params[2]
C = best_params[3]
D = best_params[4]
#Make a matrix. 20 rows for 20 overs, 10 columns for the 0-9 wickets taken # NA for cell values to start so that
# we know if we missed something because it will still be NA
newDLT = matrix(NA, nrow=20, ncol=10)
# Compute the matrix row by row, where each row is an over
for(overcount in 1:20) {
  # Apply the example formula. 1 - (formula) because resource = 1 - proportion.
  newDLT[overcount,] = 1 - (A*overcount^2 + B*(0:9)^2 + C*overcount^3 + D*overcount*(0:9)) 
  }
# Compare
newDLT
```

Smoothing out values to get our final DLS Table.

```{r message=FALSE, warning=FALSE}
#smoothing out values
for(loopcount in 1:10)
{
  
  temp = rbind(newDLT[2:20,], rep(0, 10))
  newDLT = pmax(temp, newDLT)
}

range = max(newDLT) - min(newDLT)
newDLT2 = (newDLT - min(newDLT)) / range
newDLT2
```

Final DLT table values, rounding off to 3 decimal places

```{r message=FALSE, warning=FALSE}
# Final results of our DLT table values
round(newDLT2,3)
```

As we can see above, our values are pretty close to the values in the original DLT table which is present below.

```{r message=FALSE, warning=FALSE}
#Actual DLT
DLT
```